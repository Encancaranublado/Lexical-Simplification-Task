{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbt3S7v6xvjO"
   },
   "source": [
    "IMPORTING ALL ESSENTIAL LIBRARIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlHM50GPYD0R"
   },
   "source": [
    "In order to avoid any version compatibility issues:\n",
    "- run the following code\n",
    "- restart the session\n",
    "- run the rest of the code\n",
    "\n",
    "- do NOT rerun the code just below after restarting the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXEf5752zlbR",
    "outputId": "14473649-d1de-4850-d9d9-95b25cf82e2f"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip uninstall -y scipy gensim\n",
    "!pip install --no-cache-dir scipy==1.10.1 gensim==4.3.1 transformers==4.36.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6K25TEnxKVT",
    "outputId": "3ca614e7-8233-48b9-91c9-79693bc7b163"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from gensim.models import KeyedVectors\n",
    "import torch\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWK6gtmZx1RX"
   },
   "source": [
    "IMPORTING FILES FROM DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VkreI5Ox44d",
    "outputId": "1b9cf8bf-91a9-4b23-acda-852a0040df79"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/MyDrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbL-pae-yZYV"
   },
   "source": [
    "DEFINING ALL THE PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfvIvfrEybQB"
   },
   "outputs": [],
   "source": [
    "combined_labels_spa = '/content/MyDrive/MyDrive/Computational Linguistics/Lexical Simplification/multils_test_spanish_combined_labels.tsv'\n",
    "\n",
    "subtlex_es = '/content/MyDrive/MyDrive/Computational Linguistics/Lexical Simplification/spa-pr_web_2016_100K-words.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0MSR5JYyc8Y"
   },
   "source": [
    "PREPROCESSING OF THE SUBTLEX ES DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cVmM1rayjs8"
   },
   "outputs": [],
   "source": [
    "# Building a dictionary with the word frecuency data we need:\n",
    "\n",
    "word_frequency_dict = {}\n",
    "\n",
    "with open(subtlex_es, 'r', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    parts = line.strip().split('\\t')\n",
    "    if len(parts) == 3:\n",
    "      _, word, frequency = parts\n",
    "      lemma = word.lower()\n",
    "      if re.match(r'^[A-Za-záéíóúñüÁÉÍÓÚÑÜ]+$', lemma):\n",
    "        word_frequency_dict[lemma] = int(frequency)\n",
    "\n",
    "# We wanna keep only words so we're gonna filter out punctuation using regex.\n",
    "\n",
    "word_frequency_dict = {word: frequency for word, frequency in word_frequency_dict.items() if re.match(r'^\\w+$', word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIFfrR-Ky2f-",
    "outputId": "d5f14325-85d4-44e2-cf05-47dbe79f2132"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRNCEfrR1sRZ",
    "outputId": "e4227e70-acc8-4475-ba91-4e2d45015685"
   },
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\n",
    "!gunzip cc.es.300.vec.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFageflW7YmZ"
   },
   "source": [
    "ORGANIZING AND PREPROCESSING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UU7x-vakylAd"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "# Loading the FastText model:\n",
    "\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('cc.es.300.vec')\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Lemmatization function:\n",
    "\n",
    "def lemmatize(word):\n",
    "  document = nlp(word)\n",
    "  return document[0].lemma_ if document else word\n",
    "\n",
    "# Clean every single word:\n",
    "\n",
    "def clean_word(word):\n",
    "  return re.sub(r'[^\\wáéíóúñüÁÉÍÓÚÑÜ ]', '', word.lower().strip())\n",
    "\n",
    "# Cleaning the candidate words list:\n",
    "\n",
    "def clean_candidates(row):\n",
    "    candidates = row[5:]\n",
    "    cleaned = [clean_word(word) for word in candidates if pd.notnull(word) and str(word).strip()]\n",
    "    return list(set(cleaned))\n",
    "\n",
    "df_raw = pd.read_csv(combined_labels_spa, sep='\\t', encoding='utf-8')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'sentence': df_raw['context'].str.strip(),\n",
    "    'target_word': df_raw['target'].apply(clean_word),\n",
    "    'complexity': df_raw['complexity'],\n",
    "    'candidate_words': df_raw.apply(clean_candidates, axis=1)\n",
    "})\n",
    "\n",
    "# Function to generate wordnet substitutions for Spanish:\n",
    "\n",
    "def gen_fasttext_subs(word, top_k=5, min_frequency=1):\n",
    "  \"\"\"\n",
    "  - this function lemmatized both the words and the candidates\n",
    "  - it removes target lemma from the output\n",
    "  - it filters the candidates by frequency, stopwords, length and duplication\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "  # Getting similar words:\n",
    "    target_lemma = lemmatize(word)\n",
    "    target_length = len(word)\n",
    "    similar_words = fasttext_model.most_similar(word.lower(), topn=top_k * 5)\n",
    "\n",
    "    candidates = []\n",
    "    for candidate, score in similar_words:\n",
    "      lemma = lemmatize(candidate)\n",
    "      word_clean = candidate.lower()\n",
    "\n",
    "      if lemma == target_lemma:\n",
    "        continue\n",
    "\n",
    "      if word_frequency_dict.get(lemma, 0) < min_frequency:\n",
    "        continue\n",
    "\n",
    "      if word_clean in STOP_WORDS or not word_clean.isalpha():\n",
    "        continue\n",
    "\n",
    "      if len(word_clean) > target_length * 1.2:\n",
    "        continue\n",
    "\n",
    "      candidates.append((lemma, score))\n",
    "\n",
    "      if len(candidates) >= top_k * 5:\n",
    "        break\n",
    "\n",
    "    # threshold will be dynamic and keep top_k by score\n",
    "    sorted_selected = [word for word, _ in sorted(candidates, key=lambda x: -x[1])]\n",
    "\n",
    "    deduplicated = list(dict.fromkeys(sorted_selected))\n",
    "\n",
    "    return deduplicated[:top_k]\n",
    "\n",
    "\n",
    "  except KeyError:\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3DS3SfH39nk"
   },
   "source": [
    "GENERATING SUBSTITUTION CANDIDATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "fQG3_pDj4ASP",
    "outputId": "895e8a61-681a-4f62-9da0-0f417898e411"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "# Generating candidates using Pre-Trained Spanish FastText Embeddings :\n",
    "\n",
    "df['generated_candidates_model_1'] = df['target_word'].apply(gen_fasttext_subs)\n",
    "\n",
    "# Now we filter the candidates by frequency using SUBTLEX:\n",
    "\n",
    "df_with_candidates = df[['target_word', 'generated_candidates_model_1']]\n",
    "\n",
    "df_with_candidates.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpNQI4gP5Pks"
   },
   "source": [
    "LOADING MODEL AND TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6a1a8bfbd93545318335139b432c2f31",
      "651500f438f44813b49b33e1045f7891",
      "252051694325435ca2c9daaac8e3eea8",
      "7a5e6355fbc641c39ab70394370e3251",
      "9c658db72cd44958805efccbb5231cb4",
      "b588e19ca79f4501b8f9bbc77fd8fe0a",
      "208710af8d634495809a31060932055a",
      "e3ab208902254eb688e908d718ba18b9",
      "62ef558c428d427bb6c1ebfa8e22ba25",
      "d18d42ee2fae48f5a95e85c87fe93b61",
      "323dbc95586141fcb23ece44f8b80e8d",
      "dce58cbf4ba9468a9b9605a4a4a1c944",
      "e1e7e902c58b41a6a49279b7c206ce4a",
      "2da82f0206ec4f44a315a975436dc19b",
      "14fcfb76a54f4bab86ecdf6fa300cf20",
      "a06552da991b4c49b6b2700af008739d",
      "c2c83acf932d4d5fbd94ac63aa9e31ec",
      "d32001ef95474d1b8c5837d9e1830da3",
      "4ff9a35adf544201bff9049d61e41387",
      "76a14634ccae4db38dc6a6706d9fae23",
      "fb3e70a550f34960900dc30b025a3227",
      "10b8f7bdd5c347608e8dcbadd54636f3",
      "6a22fdd2fa5a4ad58d138e04f7db4086",
      "97a01841373e470abddc22dd233d8b00",
      "262ab96b9c2b40d0aeafe820e7ce6048",
      "170b0d7219a04587b8a4dab25fbedd70",
      "d6b93b94e21847f0aadb75857ef1d0fd",
      "005e863f1e424013a949fe8724933061",
      "80454c9488a7435d9bb381a894d218aa",
      "519cde721ed147c48aa42b0c745bb145",
      "54d6472ba0ff4d0bb9650dfcfb20f7fc",
      "952ac997a6b94bd7bad1cdea48e9b4c6",
      "5215501a3f004d8c848e8a60e0c57302",
      "81342faa6d08418ba1cf7ba0e8bfa9b8",
      "eec6d7fee2b54e78bc4a1df558fd9fe8",
      "73eed13afba240aaaf3c9ef0e043f73c",
      "b402e7269fbf460a9bb4cb78ad65a2c0",
      "e7f3d9e9f889427383ba0a0cef307ea9",
      "06355497293641568b8d475ff8d1c516",
      "3d1e13388f794531883b5ec03515ef0b",
      "f319abc3a8c04849bd6e6b273ef67323",
      "995fdbdaa10e4ee2bc0f58ea76efc9b0",
      "6bd8ef7edd5c414880566518a720acc2",
      "a4b1153bd84445f399e36c55b38037da",
      "a0a1ea9df11c475a987902b65423c937",
      "79cb0736f8bd45edbedad47466deaa62",
      "ff62802e90414ccbab82b3a52ee105ca",
      "3fa1b16d5f6e4c778f6966f47552b324",
      "e11a6fe6a602407097d174a8ef97e8d8",
      "3d3f31fb822e415d87448616e09ebdd4",
      "4c4e96c7d6dd4c069d4a82e5febdc877",
      "771b680c1573412cb48f8e8d9a233fe3",
      "af8e0d1d3b40468c915aa00faebaa5ab",
      "ab60a897b709409aae275964403ad066",
      "2a63946b4e594319b2cff657366a6e36",
      "bc4fecd6d9714d509972e4ab3286c624",
      "66d9d34768274563ab21e06e5a1ac2a2",
      "f3f3157d1f5f44bcb3393883afc6f068",
      "b82db87dd9144e1f90a90b2411958758",
      "156d386451634a8d8bd3f62cd86f5ede",
      "ffb688e0e4804bc4ab2003ccbddf1986",
      "9fb6a5e49f6b4b3bb8bd262b130fcf35",
      "2c1992016be342d9a46ec3425c16a13f",
      "62ac182f7cd14f709711f9587ac836d1",
      "d836f2e7545a4f3d93874dede5e11349",
      "907946a6541045d09615891a73eb9946",
      "2721a6e281e9434ea338509a0237a818",
      "db32366a3434489880526eebf65521cf",
      "2be7df4ed7de41efa202026269409039",
      "c0b2cc4e098b49eeb07182aa371d692c",
      "afd4ddde43e749eda596ef28076fe428",
      "6c9f3c86e3a5468bb3c8816a768bfc09",
      "a1529a63d9c94a58b74a276941d29677",
      "9361356503374b51adef0bea128829f5",
      "8246b3f5cbeb4a94824da77b87f88490",
      "e2d6768feda849388632e22a3c29629d",
      "cba022522dea49baa82966d5706753b8"
     ]
    },
    "id": "x51DXMaM5Sn8",
    "outputId": "d879b7a2-809a-49b6-e8a3-ecf9c2262ae2"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('PlanTL-GOB-ES/roberta-base-bne')\n",
    "model = AutoModelForMaskedLM.from_pretrained('PlanTL-GOB-ES/roberta-base-bne')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDzox7dX5Y6B"
   },
   "source": [
    "DEFINING A FUNCTION TO GENERATE SUBSITUTION CANDIDATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQLecVR75YEq"
   },
   "outputs": [],
   "source": [
    "def gen_mlm_subs(sentence, target_word, top_k=5):\n",
    "  # This function will replace the target word with [MASK] and use MLM to predict\n",
    "  # its substitutions.\n",
    "\n",
    "  clean_target = clean_word(target_word)\n",
    "  pattern = re.compile(rf'\\b{re.escape(clean_target)}\\b', re.IGNORECASE)\n",
    "\n",
    "  if not pattern.search(sentence):\n",
    "    return []\n",
    "\n",
    "  # Replacing only the first occurence:\n",
    "\n",
    "  masked = pattern.sub(tokenizer.mask_token, sentence, count=1)\n",
    "  inputs = tokenizer(masked, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "  mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "  logits = outputs.logits[0, mask_token_index, :]\n",
    "  token_ids = torch.topk(logits, top_k * 5, dim=1).indices[0].tolist()\n",
    "\n",
    "  # POS and lemmatization filtering\n",
    "\n",
    "  target_doc = nlp(clean_target)\n",
    "  if not target_doc:\n",
    "    return []\n",
    "\n",
    "  target_lemma = target_doc[0].lemma_.lower()\n",
    "  target_pos = target_doc[0].pos_\n",
    "\n",
    "  candidates = []\n",
    "  for token_id in token_ids:\n",
    "    # Preventing artifacts for better decoding:\n",
    "    word = tokenizer.decode([token_id], skip_special_tokens=True).strip()\n",
    "    word_clean = clean_word(word)\n",
    "    if not word_clean:\n",
    "      continue\n",
    "\n",
    "    doc = nlp(word_clean)\n",
    "\n",
    "    if not doc:\n",
    "      continue\n",
    "\n",
    "    lemma = doc[0].lemma_.lower()\n",
    "    pos = doc[0].pos_\n",
    "\n",
    "    if lemma == target_lemma or pos != target_pos:\n",
    "        continue\n",
    "\n",
    "    if word_clean in STOP_WORDS or not word_clean.isalpha():\n",
    "      continue\n",
    "\n",
    "    if len(word_clean) > len(clean_target) * 1.2:\n",
    "      continue\n",
    "\n",
    "    if word_frequency_dict.get(lemma, 0) < 1:\n",
    "      continue\n",
    "\n",
    "    candidates.append(lemma)\n",
    "\n",
    "    if len(candidates) >= top_k:\n",
    "      break\n",
    "\n",
    "  return list(dict.fromkeys(candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTjYV2ps5i_C"
   },
   "source": [
    "GENERATING..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "tcbd3B0E5l6h",
    "outputId": "857e5bf4-bc09-4380-bc72-047c596fa763"
   },
   "outputs": [],
   "source": [
    "df['generated_candidates_model_bert'] = df.apply(lambda row: gen_mlm_subs(row['sentence'], row['target_word']), axis=1)\n",
    "\n",
    "# Preview:\n",
    "\n",
    "df_with_generated_bert = df[['sentence', 'target_word', 'generated_candidates_model_bert']]\n",
    "\n",
    "df_with_generated_bert.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBht9WbP_oKB"
   },
   "source": [
    "NOW: FOR THE TASK OF SUBSTITUTION SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SfQMqyi_zYt"
   },
   "source": [
    "We will implement classic filter-based selection for the first model approach, and an embedding-based semantic selection approach for the second model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WQmZzB1AStc"
   },
   "source": [
    "This is a function that will select candidate words by filtering through PoS, frequency and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-to1nMSX_xpt"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_antonyms(word):\n",
    "  \"\"\"\n",
    "  This function will return a set of antonyms in Spanish for the target word.\n",
    "  This will help us filter out any antonyms in the code.\n",
    "  \"\"\"\n",
    "  antonyms = set()\n",
    "  for syn in wordnet.synsets(word, lang='spa'):\n",
    "    for lemma in syn.lemmas(lang='spa'):\n",
    "      for antonym in lemma.antonyms():\n",
    "        antonyms.add(antonym.name().lower())\n",
    "  return antonyms\n",
    "\n",
    "def select_model_1(target_word, sentence, candidates, fasttext_model, nlp, window_size=2, top_k=5):\n",
    "  \"\"\"\n",
    "  - this function will rank substitution candidates based on their semantic similarity\n",
    "  to the sentence words around the target word.\n",
    "  \"\"\"\n",
    "  antonyms = get_antonyms(target_word)\n",
    "\n",
    "\n",
    "  doc = nlp(sentence)\n",
    "  tokens = [token.text.lower() for token in doc]\n",
    "\n",
    "  # Finding the index of the target word:\n",
    "\n",
    "  try:\n",
    "    idx = tokens.index(target_word.lower())\n",
    "  except ValueError:\n",
    "    return []\n",
    "\n",
    "  # Constructing a context windows around the target word that EXCLUDES the target word:\n",
    "\n",
    "  start = max(0, idx - window_size)\n",
    "  end = min(len(tokens), idx + window_size + 1)\n",
    "  context_words = tokens[start:idx] + tokens[idx+1:end]\n",
    "\n",
    "  # Mean context vector\n",
    "\n",
    "  context_vectors = [fasttext_model[word] for word in context_words if word in fasttext_model]\n",
    "  if not context_vectors:\n",
    "    return []\n",
    "  context_vector = np.mean(context_vectors, axis=0)\n",
    "\n",
    "  # Scoring each candidate by its cosine similarity to the context vector\n",
    "\n",
    "  scored = []\n",
    "  for candidate in candidates:\n",
    "    candidate_lower = candidate.lower()\n",
    "    if candidate_lower not in fasttext_model:\n",
    "      continue\n",
    "    if candidate_lower in antonyms:\n",
    "      continue\n",
    "    candidate_vector = fasttext_model[candidate_lower].reshape(1, -1)\n",
    "    similarity = cosine_similarity(context_vector.reshape(1, -1), candidate_vector)[0][0]\n",
    "    scored.append((candidate, similarity))\n",
    "\n",
    "  # Sorting and returning top_k:\n",
    "\n",
    "  scored.sort(key=lambda x: -x[1])\n",
    "  return [candidate for candidate, _ in scored[:top_k]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aXkuBPuBWGY"
   },
   "source": [
    "Now applying the function to the candidates generated by the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "yGN8SnznBew6",
    "outputId": "fad33285-3274-4454-e869-b169c7137540"
   },
   "outputs": [],
   "source": [
    "df['selected_candidates_1'] = df.apply(lambda row: select_model_1(row['target_word'],\n",
    "                                                                  row['sentence'],\n",
    "                                                                  row['generated_candidates_model_1'],\n",
    "                                                                  fasttext_model,\n",
    "                                                                  nlp,\n",
    "                                                                  window_size=2,\n",
    "                                                                  top_k=5), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_selected_1 = df[['target_word', 'generated_candidates_model_1', 'selected_candidates_1']]\n",
    "\n",
    "df_selected_1.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrDGieauC5os"
   },
   "source": [
    "Substitution Selection via Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSM-iQc2dEcr"
   },
   "outputs": [],
   "source": [
    "MISTRAL_API_KEY = 'z5HoxSvQiMUrFg4KXIoUDi6arrkGuBD3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZx3WyeHDTrA"
   },
   "outputs": [],
   "source": [
    "from ast import pattern\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from requests.models import Response\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def select_model_2(sentence, target_word, candidates, top_k=5):\n",
    "  \"\"\"\n",
    "  - this function will prompt Mistral to reorder the candidate words based on how well they'd fit on the masked space.\n",
    "  \"\"\"\n",
    "\n",
    "  pattern = re.compile(rf'\\b{re.escape(target_word)}\\b', re.IGNORECASE)\n",
    "  masked_sentence = pattern.sub('____', sentence, count=1)\n",
    "  antonyms = get_antonyms(target_word) or []\n",
    "\n",
    "  # Constructing the prompt en español:\n",
    "\n",
    "  prompt = (\n",
    "      f\"Dada la oración incompleta: \\n\\n\"\n",
    "      f\"\\\"{masked_sentence}\\n\\n\"\n",
    "      f\"Y las siguientes palabras: {candidates}\\n\\n\"\n",
    "      f\"Ordena las palabras desde la más simple y adecuada (1) a la menos adecuada o la menos simple {len(candidates)} \"\n",
    "      f\"según lo bien que encajarían en el espacio en blanco y según lo fácil que fuesen entendidas por un lector promedio.\"\n",
    "  )\n",
    "\n",
    "  # Mistral API\n",
    "\n",
    "  url = 'https://api.mistral.ai/v1/chat/completions'\n",
    "  headers = {\n",
    "      'Authorization': f'Bearer {MISTRAL_API_KEY}',\n",
    "      'Content-Type': 'application/json'\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "      'model': 'mistral-small',\n",
    "      'messages': [\n",
    "          {\n",
    "              'role': 'user',\n",
    "              'content': prompt\n",
    "          }\n",
    "      ],\n",
    "      'temperature': 0.0,\n",
    "      'max_tokens': 200\n",
    "  }\n",
    "\n",
    "  response = requests.post(url, headers=headers, json=payload)\n",
    "  response.raise_for_status()\n",
    "  result = response.json()\n",
    "\n",
    "  # Extracting reply\n",
    "\n",
    "  reply = result['choices'][0]['message']['content']\n",
    "\n",
    "  # Next, we extract the list of candidates ordered by mistral\n",
    "  ord_candidates = []\n",
    "  for line in reply.split('\\n'):\n",
    "    for candidate in candidates:\n",
    "      if candidate in antonyms:\n",
    "        continue\n",
    "      if candidate.lower() in line.lower():\n",
    "          ord_candidates.append(candidate)\n",
    "          break\n",
    "\n",
    "  return ord_candidates[:top_k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT1tDJvoETzj"
   },
   "source": [
    "Now we apply to the RoBERTa MLM candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "VCa2YUH1mzF7",
    "outputId": "dfad7797-6e85-4982-a6ab-8594e9fe1f86"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def selecting_model_2(row):\n",
    "  result = select_model_2(sentence=row['sentence'], target_word=row['target_word'], candidates=row['generated_candidates_model_bert'])\n",
    "  time.sleep(2)\n",
    "  return result\n",
    "\n",
    "df['selected_candidates_2'] = df.apply(selecting_model_2, axis=1)\n",
    "\n",
    "df_selected_2 = df[['target_word', 'generated_candidates_model_bert', 'selected_candidates_2']]\n",
    "\n",
    "df_selected_2.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGMGmSYP0T4S"
   },
   "outputs": [],
   "source": [
    "df_selected_2.to_csv(\"selected_candidates_mslp2024.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiJykCZLPSE1"
   },
   "source": [
    "EVALUATION OF THE RESULTS FOR SUBSTITUTION GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBBAi57Ir8RS"
   },
   "source": [
    "DEFINING THE METRIC FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-xk7VeIubf-"
   },
   "outputs": [],
   "source": [
    "gold_df = pd.read_csv(combined_labels_spa, sep='\\t', encoding='utf-8')\n",
    "\n",
    "sub_cols = [col for col in gold_df.columns if col.startswith('substitution_')]\n",
    "\n",
    "def extract_gold_substitutes(row):\n",
    "  return set(\n",
    "      str(word).strip().lower() for word in row[5:]\n",
    "      if isinstance(word, str) and word.strip())\n",
    "\n",
    "gold_df['gold_substitutes'] = gold_df.apply(extract_gold_substitutes, axis=1)\n",
    "\n",
    "gold_subs = gold_df[['target', 'gold_substitutes']].rename(columns={'target': 'target_word'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTNHDl5fr6Bb"
   },
   "outputs": [],
   "source": [
    "def precision_at_k(selected, gold, k=10):\n",
    "  selected = selected[:k]\n",
    "  return len(set(selected) & gold) / k if k else 0.0\n",
    "\n",
    "def recall_at_k(selected, gold, k=10):\n",
    "  return len(set(selected) & gold) / len(gold) if gold else 0.0\n",
    "\n",
    "def f1_score_at_k(p, r):\n",
    "  return 2 * p * r / (p + r) if p + r else 0.0\n",
    "\n",
    "def potential_at_k(selected, gold, k=10):\n",
    "  return int(len(set(selected[:k]) & gold) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_dpvibIsn8m"
   },
   "source": [
    "EVALUATION FUNCTION FOR GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZCK2YDTu3Hi"
   },
   "outputs": [],
   "source": [
    "def evaluate_substitution_generation_clean(df, pred_col, gold_col='gold_substitutes', k=10):\n",
    "    precision_scores = []\n",
    "    recall_scores    = []\n",
    "    f1_scores        = []\n",
    "    potential_scores = []\n",
    "    valid_rows       = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # normalize\n",
    "        predicted = [w.strip().lower() for w in row[pred_col] if isinstance(w, str)]\n",
    "        if not isinstance(row[gold_col], (set, list)):\n",
    "          continue\n",
    "        gold = {w.strip().lower() for w in row[gold_col] if isinstance(w, str)}\n",
    "\n",
    "        if not gold or not predicted:\n",
    "            continue\n",
    "\n",
    "        valid_rows += 1\n",
    "        p = precision_at_k(predicted, gold, k)\n",
    "        r = recall_at_k(predicted, gold, k)\n",
    "        f1 = f1_score_at_k(p, r)\n",
    "        pot = potential_at_k(predicted, gold, k)\n",
    "\n",
    "        precision_scores.append(p)\n",
    "        recall_scores.append(r)\n",
    "        f1_scores.append(f1)\n",
    "        potential_scores.append(pot)\n",
    "\n",
    "    if valid_rows == 0:\n",
    "        return {\n",
    "            \"Precision at 10\": 0.0,\n",
    "            \"Recall at 10\"   : 0.0,\n",
    "            \"F1 Score at 10\": 0.0,\n",
    "            \"Potential at 10\": 0.0\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"Precision at 10\": sum(precision_scores) / valid_rows,\n",
    "        \"Recall at 10\"   : sum(recall_scores)    / valid_rows,\n",
    "        \"F1 Score at 10\": sum(f1_scores)        / valid_rows,\n",
    "        \"Potential at 10\": sum(potential_scores) / valid_rows\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TldyuHGRD12D"
   },
   "source": [
    "EVALUATION FUNCTION FOR SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peTzB1tkd7EI"
   },
   "outputs": [],
   "source": [
    "def evaluate_selection(df, pred_col, gold_col='gold_substitutes', k=5):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    mrr_scores = []\n",
    "    valid_rows = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        predicted = [w.strip().lower() for w in row[pred_col] if isinstance(w, str)]\n",
    "        if not isinstance(row[gold_col], (set, list)):\n",
    "            continue\n",
    "        gold = {w.strip().lower() for w in row[gold_col] if isinstance(w, str)}\n",
    "\n",
    "        if not gold or not predicted:\n",
    "            continue\n",
    "\n",
    "        valid_rows += 1\n",
    "\n",
    "        # Precision, Recall, F1\n",
    "        p = precision_at_k(predicted, gold, k)\n",
    "        r = recall_at_k(predicted, gold, k)\n",
    "        f1 = f1_score_at_k(p, r)\n",
    "\n",
    "        precision_scores.append(p)\n",
    "        recall_scores.append(r)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # MRR (Mean Reciprocal Rank)\n",
    "        reciprocal_ranks = []\n",
    "        for idx, candidate in enumerate(predicted[:k]):\n",
    "            if candidate in gold:\n",
    "                reciprocal_ranks.append(1 / (idx + 1))\n",
    "        mrr = max(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "        mrr_scores.append(mrr)\n",
    "\n",
    "    if valid_rows == 0:\n",
    "        return {\n",
    "            \"Precision at 5\": 0.0,\n",
    "            \"Recall at 5\": 0.0,\n",
    "            \"F1 Score at 5\": 0.0,\n",
    "            \"MRR at 5\": 0.0\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"Precision at 5\": sum(precision_scores) / valid_rows,\n",
    "        \"Recall at 5\": sum(recall_scores) / valid_rows,\n",
    "        \"F1 Score at 5\": sum(f1_scores) / valid_rows,\n",
    "        \"MRR at 5\": sum(mrr_scores) / valid_rows\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9s0__dXX5kG"
   },
   "source": [
    "EVALUATION OF SUBSTITUTION SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHP9lbDbf0n8"
   },
   "outputs": [],
   "source": [
    "for df_ in [df_with_candidates, df_with_generated_bert, df_selected_1, df_selected_2]:\n",
    "    if 'gold_substitutes' in df_.columns:\n",
    "        df_.drop(columns=['gold_substitutes'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATgjsGVsgAkR"
   },
   "outputs": [],
   "source": [
    "df_with_candidates = df_with_candidates.merge(gold_subs, on='target_word', how='left')\n",
    "df_with_generated_bert = df_with_generated_bert.merge(gold_subs, on='target_word', how='left')\n",
    "df_selected_1 = df_selected_1.merge(gold_subs, on='target_word', how='left')\n",
    "df_selected_2 = df_selected_2.merge(gold_subs, on='target_word', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxSR_Xmx7BIP",
    "outputId": "f52b2a87-9360-458e-a79d-8ccb106f7cce"
   },
   "outputs": [],
   "source": [
    "for name, df_ in [('df_with_candidates', df_with_candidates),\n",
    "                  ('df_with_generated_bert', df_with_generated_bert),\n",
    "                  ('df_selected_1', df_selected_1),\n",
    "                  ('df_selected_2', df_selected_2)]:\n",
    "    print(f\"{name}: gold_substitutes in columns? {'gold_substitutes' in df_.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lFtd-4Fhlwg"
   },
   "outputs": [],
   "source": [
    "# Generation Metrics\n",
    "metrics_gen_fasttext = evaluate_substitution_generation_clean(df_with_candidates, 'generated_candidates_model_1')\n",
    "metrics_gen_bert     = evaluate_substitution_generation_clean(df_with_generated_bert, 'generated_candidates_model_bert')\n",
    "\n",
    "# Selection Metrics\n",
    "metrics_sel_1 = evaluate_selection(df_selected_1, 'selected_candidates_1')\n",
    "metrics_sel_2 = evaluate_selection(df_selected_2, 'selected_candidates_2')\n",
    "\n",
    "# Pretty Print Function\n",
    "def pretty_print_metrics(title, metrics):\n",
    "    print(f\"\\n📊 {title}\")\n",
    "    print(\"=\" * (len(title) + 4))\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k:<20}: {v * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BY9ze5mZhpD9",
    "outputId": "279dd10d-f72a-4c25-f60a-484b3f71de71"
   },
   "outputs": [],
   "source": [
    "pretty_print_metrics(\"FastText Generation Metrics\", metrics_gen_fasttext)\n",
    "pretty_print_metrics(\"RoBERTa MLM Generation Metrics\", metrics_gen_bert)\n",
    "\n",
    "pretty_print_metrics(\"Filtering Selection Metrics\", metrics_sel_1)\n",
    "pretty_print_metrics(\"RoBERTa Selection Metrics\", metrics_sel_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY9bVK0Yq7FI"
   },
   "source": [
    "ERROR ANALYSIS: SOME OBSERVATIONS ABOUT WHAT'S GOING ON BEHIND THE SCENES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q34EDIL8LFgD"
   },
   "source": [
    "PER-INSTANCE ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov4R1wg3rIRl"
   },
   "outputs": [],
   "source": [
    "def get_prediction_outcomes(row, pred_col, gold_col='gold_substitutes', k=10):\n",
    "  preds = [word.strip().lower() for word in row[pred_col] if isinstance(word, str)][:k]\n",
    "\n",
    "  gold_raw = row[gold_col]\n",
    "  if not isinstance(gold_raw, (set, list)):\n",
    "    return pd.Series({\n",
    "        'target_word': row['target_word'],\n",
    "        'sentence': row.get('sentence', ''),\n",
    "        'predictions': preds,\n",
    "        'gold_substitutes': [],\n",
    "        'true_positives': [],\n",
    "        'false_positives': preds,\n",
    "        'false_negatives': [],\n",
    "        'TP_count': 0,\n",
    "        'FP_count': len(preds),\n",
    "        'FN_count': 0,\n",
    "        'potential_hit': 0\n",
    "    })\n",
    "\n",
    "  golds = {word.strip().lower() for word in row[gold_col] if isinstance(word, str)}\n",
    "\n",
    "  tp = [word for word in preds if word in golds] # True positives\n",
    "  fp = [word for word in preds if word not in golds] # False positives\n",
    "  fn = [word for word in golds if word not in preds] # False negatives\n",
    "\n",
    "  return pd.Series({\n",
    "      'target_word': row['target_word'],\n",
    "      'sentence': row.get('sentence', ''),\n",
    "      'predictions': preds,\n",
    "      'gold_substitutes': list(golds),\n",
    "      'true_positives': tp,\n",
    "      'false_positives': fp,\n",
    "      'false_negatives': fn,\n",
    "      'TP_count': len(tp),\n",
    "      'FP_count': len(fp),\n",
    "      'FN_count': len(fn),\n",
    "      'potential_hit': int(len(tp) > 0)\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sQA0XqlZMHVs",
    "outputId": "80e1101d-8569-42e3-b4ba-d45a7c5d96f4"
   },
   "outputs": [],
   "source": [
    "# Error analysis for generation:\n",
    "\n",
    "error_analysis_gen_fasttext = df_with_candidates.apply(lambda row: get_prediction_outcomes(row, 'generated_candidates_model_1'), axis=1)\n",
    "error_analysis_gen_bert = df_with_generated_bert.apply(lambda row: get_prediction_outcomes(row, 'generated_candidates_model_bert'), axis=1)\n",
    "\n",
    "error_analysis_gen_fasttext[error_analysis_gen_fasttext['TP_count'] == 0].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bt3kGvlfNfRm",
    "outputId": "032d919e-ed96-46ec-85bf-926167135fe5"
   },
   "outputs": [],
   "source": [
    "error_analysis_gen_bert[error_analysis_gen_bert['TP_count'] == 0].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MzHZsr0487J5",
    "outputId": "22ccb060-b17f-4b37-ed68-1d9f9afc9da5"
   },
   "outputs": [],
   "source": [
    "error_analysis_selec_fasttext = df_selected_1.apply(lambda row: get_prediction_outcomes(row, 'selected_candidates_1'), axis=1)\n",
    "error_analysis_selec_bert = df_selected_2.apply(lambda row: get_prediction_outcomes(row, 'selected_candidates_2'), axis=1)\n",
    "\n",
    "error_analysis_selec_fasttext[error_analysis_selec_fasttext['TP_count'] == 0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GWbPcpfj9Eb7",
    "outputId": "2fb7c3d4-d939-41d1-a6e7-f5ed2b193687"
   },
   "outputs": [],
   "source": [
    "error_analysis_selec_bert[error_analysis_selec_bert['TP_count'] == 0].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thKiZfDzcG6y"
   },
   "source": [
    "TP vs. FP Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "6fqeR_PmcGrW",
    "outputId": "0afb2ea2-cd43-4040-b220-b6c690e4ed90"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tp_fp_heatmap(error_df, model_name=\"Model\"):\n",
    "    # Create a pivot table (confusion-like matrix)\n",
    "    matrix = error_df.groupby(['TP_count', 'FP_count']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='YlOrRd', linewidths=0.5)\n",
    "    plt.title(f\"📊 TP vs FP Heatmap — {model_name}\")\n",
    "    plt.xlabel(\"False Positives (FP)\")\n",
    "    plt.ylabel(\"True Positives (TP)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_tp_fp_heatmap(error_analysis_gen_fasttext, \"FastText (Generation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "w9uuZoCFdf3W",
    "outputId": "c7042ba8-5df8-44cc-dcc9-796dfa57c263"
   },
   "outputs": [],
   "source": [
    "plot_tp_fp_heatmap(error_analysis_gen_bert, \"RoBERTa (Generation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7v02UOPxYhO"
   },
   "source": [
    "SOME ERROR ANALYSIS FOR SUBSTITUTION SELECTION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hgUPnQ_xdwK"
   },
   "outputs": [],
   "source": [
    "error_analysis_sel_1 = df_selected_1.apply(lambda row: get_prediction_outcomes(row, 'selected_candidates_1'), axis=1)\n",
    "error_analysis_sel_2 = df_selected_2.apply(lambda row: get_prediction_outcomes(row, 'selected_candidates_2'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "_an9yBvnxe6I",
    "outputId": "a8c23fee-2f57-47f3-9e87-dd34760ec943"
   },
   "outputs": [],
   "source": [
    "plot_tp_fp_heatmap(error_analysis_sel_1, \"FastText (Selection)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "id": "Vo0Qrkm1xlRo",
    "outputId": "28d9c3b6-7af0-4976-f3df-d5fe2968f1aa"
   },
   "outputs": [],
   "source": [
    "plot_tp_fp_heatmap(error_analysis_sel_2, \"RoBERTa (Selection)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "OyJz6FO1yk6q",
    "outputId": "9fe2e994-36ed-414e-c1fd-226c1e7a5e14"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_candidates = [word for sublist in df['generated_candidates_model_1'] for word in sublist]\n",
    "freqs = [word_frequency_dict.get(word, 0) for word in all_candidates]\n",
    "\n",
    "plt.hist(freqs, bins=50, log=True)\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Distribution of Frequencies in FastText Candidates')\n",
    "plt.axvline(1, color='red', linestyle='--', label='min_freq=1')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
